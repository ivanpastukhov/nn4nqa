{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Train shape: (20347, 9) \n",
      "Test shape: (3058, 9) \n",
      "Val shape (3058, 9): \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from custom_model.model import SimpleNet, SAttendedSimpleNet, SAttendedNet, CrossAttentionNet\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print('Device: ', device)\n",
    "\n",
    "def read_pickle(fname):\n",
    "    with open(fname, 'rb') as fin:\n",
    "        return pickle.load(fin)\n",
    "\n",
    "df_train = pd.read_pickle('./data/processed/wikiqa_df_train.pickle')\n",
    "df_test = pd.read_pickle('./data/processed/wikiqa_df_test.pickle')\n",
    "df_test, df_val = np.split(df_test.sample(frac=1., random_state=42), 2)\n",
    "emb_weights = np.load('./data/processed/index2vector.npy')\n",
    "\n",
    "vocab_size = emb_weights.shape[0]\n",
    "embed_dim = emb_weights.shape[1]\n",
    "\n",
    "# df_train = df_train.iloc[:100]\n",
    "\n",
    "print('Train shape: {} \\n\\\n",
    "Test shape: {} \\n\\\n",
    "Val shape {}: '.format(df_train.shape, df_test.shape, df_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext import data\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "QuestionID = data.Field(sequential=False, use_vocab=False)\n",
    "QUESTION = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "DocumentID = data.Field(sequential=False, use_vocab=False)\n",
    "DocumentTitle = data.Field(sequential=False, use_vocab=False)\n",
    "SentenceID = data.Field(sequential=False, use_vocab=False)\n",
    "SENTENCE = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "Label = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='./data/WikiQACorpus/', train='WikiQA-train.tsv',\n",
    "        validation='WikiQA-dev.tsv', test='WikiQA-test.tsv', format='tsv',\n",
    "        fields=[('QUESTIONID', QuestionID),\n",
    "                ('QUESTION', QUESTION),\n",
    "                ('DOCUMENTID', DocumentID),\n",
    "                ('DOCUMENTIDTITLE', DocumentTitle),\n",
    "                ('SENTENCEID', SentenceID),\n",
    "                ('SENTENCE', SENTENCE),\n",
    "                ('LABEL', Label)],\n",
    "        skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION.build_vocab(train, vectors=\"glove.6B.100d\")\n",
    "SENTENCE.build_vocab(train, vectors=\"glove.6B.100d\")\n",
    "Label.build_vocab(train)\n",
    "\n",
    "VOCAB = QUESTION.vocab\n",
    "VOCAB.extend(SENTENCE.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, val, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention():\n",
    "    def __init__(self, dropout=None, positional_encoding=False):\n",
    "        if dropout:\n",
    "            # TODO: dropout\n",
    "            raise NotImplementedError()\n",
    "        if positional_encoding:\n",
    "            # TODO: positional encoding\n",
    "            raise NotImplementedError()\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def self_attention(query, key, value):\n",
    "        d_k = value.size(-1)\n",
    "        score = torch.bmm(query, key.permute(0,2,1))\n",
    "        score = score / math.sqrt(d_k)\n",
    "        # TODO: потенциально слабое место с направлением softmax'a.\n",
    "        p_att = F.softmax(score, dim=-1)\n",
    "        score = torch.bmm(p_att, value)\n",
    "        return score, p_att\n",
    "    \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, emb_size, att_size=64, dropout=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        if dropout:\n",
    "            # TODO: dropout\n",
    "            raise NotImplementedError\n",
    "        self.n_heads = n_heads\n",
    "        self.emb_size = emb_size\n",
    "        self.att_size = att_size\n",
    "        self.attention = SelfAttention().self_attention\n",
    "        # (W_q) n_heads times:\n",
    "        self.linear_query = nn.ModuleList([nn.Linear(self.emb_size, self.att_size) for _ in range(self.n_heads)])\n",
    "        # (W_k) n_heads times:\n",
    "        self.linear_key = nn.ModuleList([nn.Linear(self.emb_size, self.att_size) for _ in range(self.n_heads)])\n",
    "        # (W_v) n_heads times:\n",
    "        self.linear_value = nn.ModuleList([nn.Linear(self.emb_size, self.att_size) for _ in range(self.n_heads)])\n",
    "        # Fields for keeping attended values and attention_probabilities\n",
    "        self.att_probas = []    # n_heads х n_sentences x max_len x max_len\n",
    "        self.scores = []\n",
    "        # Linear layer to transform concatenated heads\n",
    "        self.output_linear = nn.Linear(n_heads*att_size, emb_size)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # for each head:\n",
    "        for head in range(self.n_heads):\n",
    "            q = self.linear_query[head](query)\n",
    "            k = self.linear_key[head](key)\n",
    "            v = self.linear_value[head](value)\n",
    "            # Scaled dot-product attention:\n",
    "            score, p_att = self.attention(q,k,v)\n",
    "            self.att_probas.append(p_att)\n",
    "            self.scores.append(score)\n",
    "        # Concatenate resulting matrices concat(z_0, z_1, ... z__n_heads)\n",
    "        scores = torch.cat(self.scores, -1)\n",
    "        # Transform concatenated\n",
    "        scores = self.output_linear(scores)\n",
    "        # Update attention probabilities for every head\n",
    "        att_probas = self.att_probas\n",
    "        # Reset scores and probabilities\n",
    "        self.scores = []\n",
    "        self.att_probas = []\n",
    "        return scores, att_probas\n",
    "    \n",
    "class AttentionFlattener(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(AttentionFlattener, self).__init__()\n",
    "        self.attention_matrix = None\n",
    "        self.linear = nn.Linear(seq_len, 1)\n",
    "        self.softmax = nn.Softmax(0)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.attention_matrix = x\n",
    "        scores = self.linear(self.attention_matrix)\n",
    "        scores = self.softmax(scores)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(VOCAB)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = SAttendedNet(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
