{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Train shape: (20347, 9) \n",
      "Test shape: (3058, 9) \n",
      "Val shape (3058, 9): \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from custom_model.model import SimpleNet, SAttendedSimpleNet, SAttendedNet, CrossAttentionNet\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print('Device: ', device)\n",
    "\n",
    "def read_pickle(fname):\n",
    "    with open(fname, 'rb') as fin:\n",
    "        return pickle.load(fin)\n",
    "\n",
    "df_train = pd.read_pickle('./data/processed/wikiqa_df_train.pickle')\n",
    "df_test = pd.read_pickle('./data/processed/wikiqa_df_test.pickle')\n",
    "df_test, df_val = np.split(df_test.sample(frac=1., random_state=42), 2)\n",
    "emb_weights = np.load('./data/processed/index2vector.npy')\n",
    "\n",
    "vocab_size = emb_weights.shape[0]\n",
    "embed_dim = emb_weights.shape[1]\n",
    "\n",
    "# df_train = df_train.iloc[:100]\n",
    "\n",
    "print('Train shape: {} \\n\\\n",
    "Test shape: {} \\n\\\n",
    "Val shape {}: '.format(df_train.shape, df_test.shape, df_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/WikiQACorpus/WikiQA-train.tsv', sep='\\t')\n",
    "df.loc[:, ['Question', 'Sentence', 'Label']].to_csv('./data/WikiQACorpus/WikiQA-train_clear.tsv',\n",
    "                                                   sep='\\t')\n",
    "\n",
    "df = pd.read_csv('./data/WikiQACorpus/WikiQA-test.tsv', sep='\\t')\n",
    "df.loc[:, ['Question', 'Sentence', 'Label']].to_csv('./data/WikiQACorpus/WikiQA-test_clear.tsv',\n",
    "                                                   sep='\\t')\n",
    "\n",
    "df = pd.read_csv('./data/WikiQACorpus/WikiQA-dev.tsv', sep='\\t')\n",
    "df.loc[:, ['Question', 'Sentence', 'Label']].to_csv('./data/WikiQACorpus/WikiQA-dev_clear.tsv',\n",
    "                                                   sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext import data\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "QUESTION = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "SENTENCE = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='./data/WikiQACorpus/', train='WikiQA-train_clear.tsv',\n",
    "        validation='WikiQA-dev_clear.tsv', test='WikiQA-test_clear.tsv', format='tsv',\n",
    "        fields=[('LABEL', LABEL),\n",
    "                ('QUESTION', QUESTION),\n",
    "                ('SENTENCE', SENTENCE)],\n",
    "        skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LABEL': '4', 'QUESTION': ['how', 'are', 'glacier', 'caves', 'formed', '?'], 'SENTENCE': ['glacier', 'caves', 'are', 'often', 'called', 'ice', 'caves', ',', 'but', 'this', 'term', 'is', 'properly', 'used', 'to', 'describe', 'bedrock', 'caves', 'that', 'contain', 'year', '-', 'round', 'ice', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train.examples[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION.build_vocab(train, vectors=\"glove.6B.100d\")\n",
    "SENTENCE.build_vocab(train, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "VOCAB = QUESTION.vocab\n",
    "VOCAB.extend(SENTENCE.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, val, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention():\n",
    "    def __init__(self, dropout=None, positional_encoding=False):\n",
    "        if dropout:\n",
    "            # TODO: dropout\n",
    "            raise NotImplementedError()\n",
    "        if positional_encoding:\n",
    "            # TODO: positional encoding\n",
    "            raise NotImplementedError()\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def self_attention(query, key, value):\n",
    "        d_k = value.size(-1)\n",
    "        score = torch.bmm(query, key.permute(0,2,1))\n",
    "        score = score / math.sqrt(d_k)\n",
    "        # TODO: потенциально слабое место с направлением softmax'a.\n",
    "        p_att = F.softmax(score, dim=-1)\n",
    "        score = torch.bmm(p_att, value)\n",
    "        return score, p_att\n",
    "    \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, emb_size, att_size=64, dropout=None):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        if dropout:\n",
    "            # TODO: dropout\n",
    "            raise NotImplementedError\n",
    "        self.n_heads = n_heads\n",
    "        self.emb_size = emb_size\n",
    "        self.att_size = att_size\n",
    "        self.attention = SelfAttention().self_attention\n",
    "        # (W_q) n_heads times:\n",
    "        self.linear_query = nn.ModuleList([nn.Linear(self.emb_size, self.att_size) for _ in range(self.n_heads)])\n",
    "        # (W_k) n_heads times:\n",
    "        self.linear_key = nn.ModuleList([nn.Linear(self.emb_size, self.att_size) for _ in range(self.n_heads)])\n",
    "        # (W_v) n_heads times:\n",
    "        self.linear_value = nn.ModuleList([nn.Linear(self.emb_size, self.att_size) for _ in range(self.n_heads)])\n",
    "        # Fields for keeping attended values and attention_probabilities\n",
    "        self.att_probas = []    # n_heads х n_sentences x max_len x max_len\n",
    "        self.scores = []\n",
    "        # Linear layer to transform concatenated heads\n",
    "        self.output_linear = nn.Linear(n_heads*att_size, emb_size)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # for each head:\n",
    "        for head in range(self.n_heads):\n",
    "            q = self.linear_query[head](query)\n",
    "            k = self.linear_key[head](key)\n",
    "            v = self.linear_value[head](value)\n",
    "            # Scaled dot-product attention:\n",
    "            score, p_att = self.attention(q,k,v)\n",
    "            self.att_probas.append(p_att)\n",
    "            self.scores.append(score)\n",
    "        # Concatenate resulting matrices concat(z_0, z_1, ... z__n_heads)\n",
    "        scores = torch.cat(self.scores, -1)\n",
    "        # Transform concatenated\n",
    "        scores = self.output_linear(scores)\n",
    "        # Update attention probabilities for every head\n",
    "        att_probas = self.att_probas\n",
    "        # Reset scores and probabilities\n",
    "        self.scores = []\n",
    "        self.att_probas = []\n",
    "        return scores, att_probas\n",
    "    \n",
    "class AttentionFlattener(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(AttentionFlattener, self).__init__()\n",
    "        self.attention_matrix = None\n",
    "        self.linear = nn.Linear(seq_len, 1)\n",
    "        self.softmax = nn.Softmax(0)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.attention_matrix = x\n",
    "        scores = self.linear(self.attention_matrix)\n",
    "        scores = self.softmax(scores)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = VOCAB.vectors.shape[0]\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 1\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.5\n",
    "\n",
    "pretrained_embeddings = VOCAB.vectors.detach().numpy()\n",
    "model = SAttendedSimpleNet(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, HIDDEN_DIM, 1, pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.examples[4].LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        print(batch.text)\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.QUESTION, batch.SENTENCE).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.LABEL)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.LABEL)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
