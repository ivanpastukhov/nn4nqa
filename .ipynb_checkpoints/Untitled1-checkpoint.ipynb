{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n",
      "Train shape: (20347, 9) \n",
      "Test shape: (3058, 9) \n",
      "Val shape (3058, 9): \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from custom_model.model import SimpleNet, SAttendedSimpleNet, SAttendedNet, CrossAttentionNet\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print('Device: ', device)\n",
    "\n",
    "def read_pickle(fname):\n",
    "    with open(fname, 'rb') as fin:\n",
    "        return pickle.load(fin)\n",
    "\n",
    "df_train = pd.read_pickle('./data/processed/wikiqa_df_train.pickle')\n",
    "df_test = pd.read_pickle('./data/processed/wikiqa_df_test.pickle')\n",
    "df_test, df_val = np.split(df_test.sample(frac=1., random_state=42), 2)\n",
    "emb_weights = np.load('./data/processed/index2vector.npy')\n",
    "\n",
    "vocab_size = emb_weights.shape[0]\n",
    "embed_dim = emb_weights.shape[1]\n",
    "\n",
    "# df_train = df_train.iloc[:100]\n",
    "\n",
    "print('Train shape: {} \\n\\\n",
    "Test shape: {} \\n\\\n",
    "Val shape {}: '.format(df_train.shape, df_test.shape, df_val.shape))\n",
    "\n",
    "net_simple = SimpleNet(vocab_size, embed_dim, 64, emb_weights)\n",
    "# net_att = SAttendedSimpleNet(voc['voc_len'], 128, 128, 64, 3)\n",
    "net_att = SAttendedNet(vocab_size, embed_dim, 64, 32, 1, 22, 287, emb_weights)\n",
    "net_crossover = CrossAttentionNet(vocab_size, embed_dim, 64, 32, 1, 22, 287, emb_weights)\n",
    "\n",
    "Xq = np.array(df_train.Question_encoded.values.tolist())\n",
    "Xa = np.array(df_train.Sentence_encoded.values.tolist())\n",
    "t = np.array(df_train.Label.values.tolist())\n",
    "\n",
    "Xq = torch.from_numpy(Xq)\n",
    "Xa = torch.from_numpy(Xa)\n",
    "t = torch.from_numpy(t)\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xq_val = np.array(df_val.Question_encoded.values.tolist())\n",
    "Xa_val = np.array(df_val.Sentence_encoded.values.tolist())\n",
    "t_val = np.array(df_val.Label.values.tolist())\n",
    "val_data = [(torch.from_numpy(Xq_val), torch.from_numpy(Xa_val)), torch.from_numpy(t_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch: 0, loss: 0.69408. 1.7 [s] per epoch. Val loss: 0.69094\n",
      "Epoch: 1, loss: 0.68852. 1.3 [s] per epoch. Val loss: 0.68543\n",
      "Epoch: 2, loss: 0.68257. 1.2 [s] per epoch. Val loss: 0.68406\n",
      "Epoch: 3, loss: 0.68027. 1.3 [s] per epoch. Val loss: 0.69411\n",
      "Epoch: 4, loss: 0.68101. 1.1 [s] per epoch. Val loss: 0.71053\n",
      "Epoch: 5, loss: 0.67971. 1.2 [s] per epoch. Val loss: 0.69788\n",
      "Epoch: 6, loss: 0.67778. 1.2 [s] per epoch. Val loss: 0.69499\n",
      "Epoch: 7, loss: 0.67528. 1.2 [s] per epoch. Val loss: 0.69671\n",
      "Epoch: 8, loss: 0.67343. 1.2 [s] per epoch. Val loss: 0.69802\n",
      "Epoch: 9, loss: 0.67170. 1.1 [s] per epoch. Val loss: 0.70017\n",
      "Epoch: 10, loss: 0.66893. 1.2 [s] per epoch. Val loss: 0.70496\n",
      "Epoch: 11, loss: 0.66667. 1.2 [s] per epoch. Val loss: 0.71250\n",
      "Epoch: 12, loss: 0.66427. 1.2 [s] per epoch. Val loss: 0.73847\n",
      "Epoch: 13, loss: 0.66852. 1.2 [s] per epoch. Val loss: 0.70482\n",
      "Epoch: 14, loss: 0.66748. 1.1 [s] per epoch. Val loss: 0.71612\n",
      "Epoch: 15, loss: 0.65892. 1.2 [s] per epoch. Val loss: 0.71364\n",
      "Epoch: 16, loss: 0.65644. 1.2 [s] per epoch. Val loss: 0.70851\n",
      "Epoch: 17, loss: 0.65241. 1.3 [s] per epoch. Val loss: 0.70646\n",
      "Epoch: 18, loss: 0.65224. 1.4 [s] per epoch. Val loss: 0.72124\n",
      "Epoch: 19, loss: 0.64343. 1.2 [s] per epoch. Val loss: 0.71857\n",
      "Epoch: 20, loss: 0.63878. 1.3 [s] per epoch. Val loss: 0.71304\n",
      "Epoch: 21, loss: 0.63908. 1.3 [s] per epoch. Val loss: 0.69474\n",
      "Epoch: 22, loss: 0.63667. 1.2 [s] per epoch. Val loss: 0.71054\n",
      "Epoch: 23, loss: 0.63119. 1.2 [s] per epoch. Val loss: 0.70780\n",
      "Epoch: 24, loss: 0.63127. 1.2 [s] per epoch. Val loss: 0.70520\n",
      "Epoch: 25, loss: 0.63033. 1.2 [s] per epoch. Val loss: 0.70269\n",
      "Epoch: 26, loss: 0.61851. 1.3 [s] per epoch. Val loss: 0.71015\n",
      "Epoch: 27, loss: 0.61722. 1.3 [s] per epoch. Val loss: 0.70597\n",
      "Epoch: 28, loss: 0.60952. 1.3 [s] per epoch. Val loss: 0.71330\n",
      "Epoch: 29, loss: 0.61132. 1.2 [s] per epoch. Val loss: 0.70030\n",
      "Done!\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/notebooks/Ivan Pastuhov/nn4nqa/custom_model/model.py:245: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  _ = nn.utils.clip_grad_norm(self.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.69328. 2.5 [s] per epoch. Val loss: 0.69287\n",
      "Epoch: 1, loss: 0.67387. 2.3 [s] per epoch. Val loss: 0.68654\n",
      "Epoch: 2, loss: 0.63553. 2.5 [s] per epoch. Val loss: 0.67835\n",
      "Epoch: 3, loss: 0.63525. 2.5 [s] per epoch. Val loss: 0.68375\n",
      "Epoch: 4, loss: 0.62463. 2.6 [s] per epoch. Val loss: 0.68132\n",
      "Epoch: 5, loss: 0.61742. 2.5 [s] per epoch. Val loss: 0.67970\n",
      "Epoch: 6, loss: 0.61202. 2.5 [s] per epoch. Val loss: 0.67865\n",
      "Epoch: 7, loss: 0.60492. 2.4 [s] per epoch. Val loss: 0.67642\n",
      "Epoch: 8, loss: 0.59527. 2.4 [s] per epoch. Val loss: 0.67066\n",
      "Epoch: 9, loss: 0.58445. 2.6 [s] per epoch. Val loss: 0.66582\n",
      "Epoch: 10, loss: 0.58251. 2.4 [s] per epoch. Val loss: 0.66804\n",
      "Epoch: 11, loss: 0.57160. 2.5 [s] per epoch. Val loss: 0.66353\n",
      "Epoch: 12, loss: 0.56954. 2.5 [s] per epoch. Val loss: 0.66801\n",
      "Epoch: 13, loss: 0.56216. 2.5 [s] per epoch. Val loss: 0.67381\n",
      "Epoch: 14, loss: 0.56479. 2.3 [s] per epoch. Val loss: 0.67360\n",
      "Epoch: 15, loss: 0.56759. 2.3 [s] per epoch. Val loss: 0.67434\n",
      "Epoch: 16, loss: 0.55556. 2.6 [s] per epoch. Val loss: 0.67190\n",
      "Epoch: 17, loss: 0.54313. 2.5 [s] per epoch. Val loss: 0.66682\n",
      "Epoch: 18, loss: 0.52590. 2.6 [s] per epoch. Val loss: 0.66867\n",
      "Epoch: 19, loss: 0.51586. 2.6 [s] per epoch. Val loss: 0.65758\n",
      "Epoch: 20, loss: 0.51155. 2.5 [s] per epoch. Val loss: 0.66477\n",
      "Epoch: 21, loss: 0.52072. 2.5 [s] per epoch. Val loss: 0.65458\n",
      "Epoch: 22, loss: 0.51932. 2.5 [s] per epoch. Val loss: 0.65781\n",
      "Epoch: 23, loss: 0.49787. 2.6 [s] per epoch. Val loss: 0.66354\n",
      "Epoch: 24, loss: 0.50405. 2.5 [s] per epoch. Val loss: 0.65167\n",
      "Epoch: 25, loss: 0.49364. 2.5 [s] per epoch. Val loss: 0.65626\n",
      "Epoch: 26, loss: 0.48829. 2.6 [s] per epoch. Val loss: 0.66223\n",
      "Epoch: 27, loss: 0.48134. 2.5 [s] per epoch. Val loss: 0.65722\n",
      "Epoch: 28, loss: 0.48024. 2.5 [s] per epoch. Val loss: 0.65254\n",
      "Epoch: 29, loss: 0.47518. 2.5 [s] per epoch. Val loss: 0.64845\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam\n",
    "loss_func = torch.nn.CrossEntropyLoss(weight=torch.tensor([0.05, 0.95]).to(device))\n",
    "\n",
    "# net_crossover.fit(Xq, Xa, t, batch_size, epochs, loss_func, optimizer, device, 90., val_data)\n",
    "net_simple.fit(Xq, Xa, t, batch_size, epochs, loss_func, optimizer, device, None, val_data)\n",
    "net_att.fit(Xq, Xa, t, batch_size, epochs, loss_func, optimizer, device, 90., val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xq_test = np.array(df_test.Question_encoded.values.tolist())\n",
    "Xa_test = np.array(df_test.Sentence_encoded.values.tolist())\n",
    "t_test = np.array(df_test.Label.values.tolist())\n",
    "\n",
    "Xq_test = torch.from_numpy(Xq_test)\n",
    "Xa_test = torch.from_numpy(Xa_test)\n",
    "t_test = torch.from_numpy(t_test)\n",
    "\n",
    "net_simple.eval()\n",
    "net_att.eval()\n",
    "\n",
    "y_simple = net_simple.to('cpu')(Xq_test, Xa_test)\n",
    "y_attent = net_att.to('cpu')(Xq_test, Xa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC simple_net:  0.5973219350437431\n",
      "ROC_AUC attent_net:  0.6716465464032166\n"
     ]
    }
   ],
   "source": [
    "print('ROC_AUC simple_net: ', roc_auc_score(t_test, y_simple.detach().numpy().T[1]))\n",
    "print('ROC_AUC attent_net: ', roc_auc_score(t_test, y_attent.detach().numpy().T[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand((3,5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4287,  0.3942,  0.0604,  0.1245,  0.8909,  0.4458,  0.8706,\n",
       "           0.9773],\n",
       "         [ 0.1155,  0.2640,  0.1278,  0.8110,  0.4001,  0.1504,  0.0421,\n",
       "           0.0091],\n",
       "         [ 0.5653,  0.9873,  0.3048,  0.9034,  0.1659,  0.4719,  0.5392,\n",
       "           0.5871],\n",
       "         [ 0.4730,  0.2721,  0.5361,  0.7076,  0.8989,  0.2514,  0.0404,\n",
       "           0.8865],\n",
       "         [ 0.2321,  0.1911,  0.3464,  0.8846,  0.2220,  0.1765,  0.3063,\n",
       "           0.4712]],\n",
       "\n",
       "        [[ 0.6435,  0.3662,  0.1579,  0.2816,  0.7345,  0.9305,  0.7109,\n",
       "           0.6003],\n",
       "         [ 0.3210,  0.6264,  0.5718,  0.4135,  0.7088,  0.5434,  0.4712,\n",
       "           0.1546],\n",
       "         [ 0.2150,  0.2433,  0.5993,  0.1116,  0.5238,  0.6924,  0.9444,\n",
       "           0.3336],\n",
       "         [ 0.5061,  0.9467,  0.7286,  0.7167,  0.9050,  0.0789,  0.3827,\n",
       "           0.0125],\n",
       "         [ 0.9621,  0.0949,  0.8025,  0.5345,  0.5717,  0.5203,  0.8513,\n",
       "           0.0037]],\n",
       "\n",
       "        [[ 0.5968,  0.2068,  0.9622,  0.5538,  0.4449,  0.1390,  0.5541,\n",
       "           0.4626],\n",
       "         [ 0.9480,  0.1144,  0.7738,  0.3222,  0.0057,  0.8268,  0.2813,\n",
       "           0.8141],\n",
       "         [ 0.8755,  0.3231,  0.2724,  0.3659,  0.3079,  0.3166,  0.5770,\n",
       "           0.4247],\n",
       "         [ 0.1028,  0.3656,  0.7950,  0.6649,  0.1590,  0.1711,  0.9139,\n",
       "           0.6648],\n",
       "         [ 0.6196,  0.6341,  0.3063,  0.6008,  0.0171,  0.2544,  0.3165,\n",
       "           0.2676]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4287,  0.3942,  0.0604,  0.1245,  0.8909,  0.4458,  0.8706,\n",
       "           0.9773],\n",
       "         [ 0.1155,  0.2640,  0.1278,  0.8110,  0.4001,  0.1504,  0.0421,\n",
       "           0.0091],\n",
       "         [ 0.5653,  0.9873,  0.3048,  0.9034,  0.1659,  0.4719,  0.5392,\n",
       "           0.5871]],\n",
       "\n",
       "        [[ 0.4730,  0.2721,  0.5361,  0.7076,  0.8989,  0.2514,  0.0404,\n",
       "           0.8865],\n",
       "         [ 0.2321,  0.1911,  0.3464,  0.8846,  0.2220,  0.1765,  0.3063,\n",
       "           0.4712],\n",
       "         [ 0.6435,  0.3662,  0.1579,  0.2816,  0.7345,  0.9305,  0.7109,\n",
       "           0.6003]],\n",
       "\n",
       "        [[ 0.3210,  0.6264,  0.5718,  0.4135,  0.7088,  0.5434,  0.4712,\n",
       "           0.1546],\n",
       "         [ 0.2150,  0.2433,  0.5993,  0.1116,  0.5238,  0.6924,  0.9444,\n",
       "           0.3336],\n",
       "         [ 0.5061,  0.9467,  0.7286,  0.7167,  0.9050,  0.0789,  0.3827,\n",
       "           0.0125]],\n",
       "\n",
       "        [[ 0.9621,  0.0949,  0.8025,  0.5345,  0.5717,  0.5203,  0.8513,\n",
       "           0.0037],\n",
       "         [ 0.5968,  0.2068,  0.9622,  0.5538,  0.4449,  0.1390,  0.5541,\n",
       "           0.4626],\n",
       "         [ 0.9480,  0.1144,  0.7738,  0.3222,  0.0057,  0.8268,  0.2813,\n",
       "           0.8141]],\n",
       "\n",
       "        [[ 0.8755,  0.3231,  0.2724,  0.3659,  0.3079,  0.3166,  0.5770,\n",
       "           0.4247],\n",
       "         [ 0.1028,  0.3656,  0.7950,  0.6649,  0.1590,  0.1711,  0.9139,\n",
       "           0.6648],\n",
       "         [ 0.6196,  0.6341,  0.3063,  0.6008,  0.0171,  0.2544,  0.3165,\n",
       "           0.2676]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(5,3,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4287,  0.3942,  0.0604,  0.1245,  0.8909,  0.4458,  0.8706,\n",
       "           0.9773],\n",
       "         [ 0.6435,  0.3662,  0.1579,  0.2816,  0.7345,  0.9305,  0.7109,\n",
       "           0.6003],\n",
       "         [ 0.5968,  0.2068,  0.9622,  0.5538,  0.4449,  0.1390,  0.5541,\n",
       "           0.4626]],\n",
       "\n",
       "        [[ 0.1155,  0.2640,  0.1278,  0.8110,  0.4001,  0.1504,  0.0421,\n",
       "           0.0091],\n",
       "         [ 0.3210,  0.6264,  0.5718,  0.4135,  0.7088,  0.5434,  0.4712,\n",
       "           0.1546],\n",
       "         [ 0.9480,  0.1144,  0.7738,  0.3222,  0.0057,  0.8268,  0.2813,\n",
       "           0.8141]],\n",
       "\n",
       "        [[ 0.5653,  0.9873,  0.3048,  0.9034,  0.1659,  0.4719,  0.5392,\n",
       "           0.5871],\n",
       "         [ 0.2150,  0.2433,  0.5993,  0.1116,  0.5238,  0.6924,  0.9444,\n",
       "           0.3336],\n",
       "         [ 0.8755,  0.3231,  0.2724,  0.3659,  0.3079,  0.3166,  0.5770,\n",
       "           0.4247]],\n",
       "\n",
       "        [[ 0.4730,  0.2721,  0.5361,  0.7076,  0.8989,  0.2514,  0.0404,\n",
       "           0.8865],\n",
       "         [ 0.5061,  0.9467,  0.7286,  0.7167,  0.9050,  0.0789,  0.3827,\n",
       "           0.0125],\n",
       "         [ 0.1028,  0.3656,  0.7950,  0.6649,  0.1590,  0.1711,  0.9139,\n",
       "           0.6648]],\n",
       "\n",
       "        [[ 0.2321,  0.1911,  0.3464,  0.8846,  0.2220,  0.1765,  0.3063,\n",
       "           0.4712],\n",
       "         [ 0.9621,  0.0949,  0.8025,  0.5345,  0.5717,  0.5203,  0.8513,\n",
       "           0.0037],\n",
       "         [ 0.6196,  0.6341,  0.3063,  0.6008,  0.0171,  0.2544,  0.3165,\n",
       "           0.2676]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(net_att.losses)), net_att.losses)\n",
    "plt.plot(range(len(net_att.val_losses)), net_att.val_losses, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_att.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xq_1 = np.array(df_train.Question_encoded.values[:5].tolist())\n",
    "xa_1 = np.array(df_train.Sentence_encoded.values[:5].tolist())\n",
    "xq_1 = torch.from_numpy(xq_1)\n",
    "xa_1 = torch.from_numpy(xa_1)\n",
    "\n",
    "pr = net_att.to('cpu')(xq_1, xa_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Question.values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Sentence.values[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sent(sent, scores):\n",
    "    sent = sent.split()\n",
    "    scores = scores.detach().numpy().flatten()\n",
    "    return pd.Series(index=sent, data=scores[:len(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_q = []\n",
    "for sent, score in zip(df_train.Question.values[:5], net_att.l_scores[:5]):\n",
    "    scoring_q.append(score_sent(sent, score))\n",
    "    \n",
    "scoring_a = []\n",
    "for sent, score in zip(df_train.Sentence.values[:5], net_att.r_scores[:5]):\n",
    "    scoring_a.append(score_sent(sent, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_q[0].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(scoring_a[3].index))\n",
    "scoring_a[3].sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[3, 'Sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in scoring_a:\n",
    "    i.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(scoring_a[3][0], scoring_a[0][1][:len(scoring_a[3][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(scoring_a[3][0], scoring_q[0][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(17,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(len(net_simple.losses)), net_simple.losses)\n",
    "plt.plot(range(len(net_simple.val_losses)), net_simple.val_losses, c='r')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(range(len(net_att.losses)), net_att.losses)\n",
    "plt.plot(range(len(net_att.val_losses)), net_att.val_losses, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax на линейное преобразование от матрицы аттеншна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((7,6))\n",
    "sm = torch.nn.Softmax(-1)\n",
    "a = sm(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = torch.nn.Linear(6, 1)\n",
    "sm_2 = torch.nn.Softmax(0)\n",
    "sm_2(vm(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.load('./data/processed/index2vector.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(t, np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-59f0fbcb2fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 Ivan Pastuhov",
   "language": "python",
   "name": "pastuhov"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
